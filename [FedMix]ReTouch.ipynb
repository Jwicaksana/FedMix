{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f014caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from unet import UNet\n",
    "from dice_loss import dice_coeff\n",
    "####################################################\n",
    "# for data splitting\n",
    "####################################################\n",
    "import pandas as pd\n",
    "####################################################\n",
    "# for data preparation\n",
    "####################################################\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, balanced_accuracy_score\n",
    "####################################################\n",
    "# for plotting\n",
    "####################################################\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "############################\n",
    "# Helper func\n",
    "############################\n",
    "from helper import * \n",
    "\n",
    "########################################\n",
    "N_CHANNELS = 1 #greyscale\n",
    "N_CLASSES = 3 # classes, IRF, SRF, PED\n",
    "\n",
    "BATCH_SIZE, EPOCHS = 16, 150\n",
    "IMAGE_SIZE = (224, 224)\n",
    "CROP_SIZE = (200, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268f6163",
   "metadata": {},
   "outputs": [],
   "source": [
    "PTH = 'dataset/retouch/mask/'\n",
    "PTH_IM = 'dataset/retouch/slices/'\n",
    "\n",
    "Cirrus_ = np.arange(1,25)\n",
    "Spectralis_ = np.arange(25,49)\n",
    "Topcon_ = np.arange(49,71)\n",
    "\n",
    "cirrus_test = np.random.choice(Cirrus_, size=5, replace=False)\n",
    "cirrus_test = [str(a) for a in cirrus_test]\n",
    "cirrus_train = [str(a) for a in Cirrus_ if str(a) not in cirrus_test]\n",
    "print(len(cirrus_train)/len(Cirrus_))\n",
    "\n",
    "spectralis_test = np.random.choice(Spectralis_, size=5, replace=False)\n",
    "spectralis_test = [str(a) for a in spectralis_test]\n",
    "spectralis_train = [str(a) for a in Spectralis_ if str(a) not in spectralis_test]\n",
    "print(len(spectralis_train)/len(Spectralis_))\n",
    "\n",
    "topcon_test = np.random.choice(Topcon_, size=5, replace=False)\n",
    "topcon_test = [str(a) for a in topcon_test]\n",
    "topcon_train = [str(a) for a in Topcon_ if str(a) not in topcon_test]\n",
    "print(len(topcon_train)/len(Topcon_))\n",
    "\n",
    "whole_data =  os.listdir(PTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feefc4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SPLIT = dict()\n",
    "TEST_SPLIT['Cirrus'] = cirrus_test\n",
    "TEST_SPLIT['Topcon'] = topcon_test\n",
    "TEST_SPLIT['Spectralis'] = spectralis_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34d0dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "WHOLE_DATA_TRAIN = dict()\n",
    "WHOLE_DATA_TEST = dict()\n",
    "\n",
    "WHOLE_DATA_TRAIN['Cirrus'] = []\n",
    "WHOLE_DATA_TRAIN['Topcon'] = []\n",
    "WHOLE_DATA_TRAIN['Spectralis'] = []\n",
    "\n",
    "WHOLE_DATA_TEST['Cirrus'] = []\n",
    "WHOLE_DATA_TEST['Topcon'] = []\n",
    "WHOLE_DATA_TEST['Spectralis'] = []\n",
    "\n",
    "for item in whole_data:\n",
    "    separator = item.split('_') # identify the source\n",
    "    sample_number = separator[1].split('0')[-1]\n",
    "    \n",
    "    '''\n",
    "    Check the item label\n",
    "    '''\n",
    "    label_slice = np.unique(np.array(Image.open(PTH+item)))\n",
    "    if len(label_slice) <=1:\n",
    "        continue\n",
    "    \n",
    "    if sample_number in TEST_SPLIT[separator[0]]:\n",
    "        # testing set\n",
    "        data_path = (PTH_IM+item, PTH+item) #x,y source\n",
    "        WHOLE_DATA_TEST[separator[0]].append(data_path)\n",
    "    else:\n",
    "        data_path = (PTH_IM+item, PTH+item) #x,y source\n",
    "        WHOLE_DATA_TRAIN[separator[0]].append(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab83d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_SPECTRALIS = 0\n",
    "print('name\\t\\t train\\t test')\n",
    "for item in WHOLE_DATA_TRAIN:\n",
    "    if item == 'Spectralis':\n",
    "        print(item,'\\t', len(WHOLE_DATA_TRAIN[item]),'\\t', len(WHOLE_DATA_TEST[item]))\n",
    "        LEN_SPECTRALIS = len(WHOLE_DATA_TRAIN[item])\n",
    "    else:\n",
    "        print(item,'\\t\\t', len(WHOLE_DATA_TRAIN[item]),'\\t', len(WHOLE_DATA_TEST[item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67948c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "WEIGHTS_CL = [0.0,0.0,0.0]\n",
    "CLIENTS = ['Cirrus', 'Topcon', 'Spectralis']\n",
    "CLIENTS_2 = [cl +'_2' for cl in CLIENTS]\n",
    "TOTAL_CLIENTS = len(CLIENTS)\n",
    "\n",
    "LR = 1.5e-3\n",
    "WD = 1e-5\n",
    "TH = 0.9\n",
    "\n",
    "LAMBDA_ =2\n",
    "BETA_=3\n",
    "TH = 0.9\n",
    "\n",
    "for idx, client in enumerate(WHOLE_DATA_TRAIN):\n",
    "    WEIGHTS_CL[idx] = len(WHOLE_DATA_TRAIN[client])\n",
    "\n",
    "    \n",
    "total_weight = sum(WEIGHTS_CL)\n",
    "WEIGHTS_CL = [s/total_weight for s in WEIGHTS_CL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8144b2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENTS_SUPERVISION = ['image', 'bbox', 'labeled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4ad07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = dict()\n",
    "for cl in CLIENTS:\n",
    "    split_dataset[cl+'_train'] = retouch(WHOLE_DATA_TRAIN[cl], train=True)\n",
    "    split_dataset[cl+'_test'] = retouch(WHOLE_DATA_TEST[cl], train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777da8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_CLIENTS = 3\n",
    "GLOBAL_ACC = 0.0\n",
    "\n",
    "training_clients, testing_clients = dict(), dict()\n",
    "########## aditional #####################\n",
    "training_clients_pl = dict()\n",
    "acc_train, acc_test, loss_train, loss_test = dict(), dict(), \\\n",
    "                                            dict(), dict()\n",
    "\n",
    "nets, optimizers = dict(), dict()\n",
    "\n",
    "nets['global'] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, \\\n",
    "                      bilinear=True).to(device)\n",
    "\n",
    "nets['global_2'] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, \\\n",
    "                      bilinear=True).to(device)\n",
    "\n",
    "for client, c_sup in zip(CLIENTS, CLIENTS_SUPERVISION):\n",
    "    if c_sup == 'labeled':\n",
    "        training_clients[client] = DataLoader(split_dataset[client+'_train'], batch_size=16,\\\n",
    "                     shuffle=True, num_workers=8)\n",
    "        training_clients_pl[client] = DataLoader(split_dataset[client+'_train'], batch_size=1, \\\n",
    "                    shuffle=True, num_workers=8)\n",
    "    else:\n",
    "        training_clients[client] = DataLoader(split_dataset[client+'_train'], batch_size=16,\\\n",
    "                             shuffle=True, num_workers=8)\n",
    "        ################# additional dataloader ##########################################\n",
    "        training_clients_pl[client] = DataLoader(split_dataset[client+'_train'], batch_size=1,\\\n",
    "                             shuffle=True, num_workers=8)\n",
    "        training_clients_pl[client+'_2'] = DataLoader(split_dataset[client+'_train'], batch_size=1,\\\n",
    "                             shuffle=True, num_workers=8)\n",
    "    ###################################################################################\n",
    "    testing_clients[client] = DataLoader(split_dataset[client+'_test'], batch_size=16,\\\n",
    "                         shuffle=False, num_workers=1)\n",
    "    \n",
    "    acc_train[client], acc_test[client] = [], []\n",
    "    loss_train[client], loss_test[client] = [], []\n",
    "        \n",
    "    nets[client] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, \\\n",
    "                      bilinear=True).to(device)\n",
    "    nets[client+'_2'] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, \\\n",
    "                      bilinear=True).to(device)\n",
    "    optimizers[client]= optim.Adam(nets[client].parameters(), \\\n",
    "                                   lr=LR,weight_decay=WD)\n",
    "    optimizers[client+'_2']= optim.Adam(nets[client+'_2'].parameters(), \\\n",
    "                                   lr=LR,weight_decay=WD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7805103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4e1531",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_CL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e8fc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_avg_acc, best_epoch_avg = 0,0\n",
    "index = []\n",
    "\n",
    "for client in CLIENTS:\n",
    "    acc_train[client], acc_test[client] = [], []\n",
    "    loss_train[client], loss_test[client] = [], [] \n",
    "    \n",
    "WEIGHTS = [0.0, 0.0, 0.0]\n",
    "DATA_NUM = [0,0, LEN_SPECTRALIS]\n",
    "score = [0,0,0]\n",
    "    \n",
    "for epoch in range(EPOCHS):\n",
    "    index.append(epoch)\n",
    "    DATA_NUM = [0,0, LEN_SPECTRALIS]\n",
    "    ####### conduct training #####\n",
    "    #################### copy fed model ###################\n",
    "    copy_fed(CLIENTS, nets, fed_name='global')\n",
    "    copy_fed(CLIENTS_2, nets, fed_name='global_2')\n",
    "    \n",
    "    \n",
    "    ######################################################\n",
    "    # generate and refine pseudo labels ##################\n",
    "    ######################################################\n",
    "    for order, client in enumerate(CLIENTS):\n",
    "        bbox, image = False, False\n",
    "        if CLIENTS_SUPERVISION[order] == 'labeled':\n",
    "            continue\n",
    "        elif CLIENTS_SUPERVISION[order] == 'bbox':\n",
    "            bbox = True\n",
    "        elif CLIENTS_SUPERVISION[order] == 'image':\n",
    "            image= True\n",
    "        ##################################################\n",
    "        # save pl ########################################\n",
    "        ##################################################\n",
    "        im_store, pl1_store, pl2_store = [], [], []\n",
    "        \n",
    "        tmp_ = select_pl_multiclasses(nets['global'], nets['global_2'], device,\\\n",
    "                      training_clients_pl[client], im_store, pl1_store, \\\n",
    "                      pl2_store, TH=TH, bbox=bbox, image=image)\n",
    "        \n",
    "        if len(im_store) >= 1:\n",
    "            tmp_dataset = retouch_v2(im_store, pl1_store, pl2_store)\n",
    "            training_clients[client] = DataLoader(tmp_dataset, batch_size=16,\\\n",
    "                             shuffle=True, num_workers=8)\n",
    "\n",
    "        DATA_NUM[order] = tmp_\n",
    "    \n",
    "    #######################################################\n",
    "    #### Conduct training #################################\n",
    "    #######################################################\n",
    "    for order, (client, supervision_t) in enumerate(zip(CLIENTS, CLIENTS_SUPERVISION)):\n",
    "        if supervision_t == 'labeled':\n",
    "            # train network 1 #\n",
    "            train_model_multiclasses(training_clients[client], nets[client], optimizers[client], device, \\\n",
    "                       acc=acc_train[client], loss=loss_train[client], \\\n",
    "                        supervision_type=supervision_t)\n",
    "            \n",
    "            # train network 2 # \n",
    "            train_model_multiclasses(training_clients[client], nets[client+'_2'], optimizers[client+'_2'], device, \\\n",
    "                       acc=None, loss=None, \\\n",
    "                        supervision_type=supervision_t)\n",
    "            \n",
    "        else: # train using pseudo label # \n",
    "            # train network 1 #\n",
    "            train_model_multiclasses(training_clients[client], nets[client], optimizers[client], device, \\\n",
    "                       acc=acc_train[client], loss=loss_train[client], \\\n",
    "                        supervision_type=supervision_t, FedMix_network=1)\n",
    "            \n",
    "            # train network 2 # \n",
    "            train_model_multiclasses(training_clients[client], nets[client+'_2'], optimizers[client+'_2'], device, \\\n",
    "                       acc=None, loss=None, \\\n",
    "                        supervision_type=supervision_t, FedMix_network=2)\n",
    "        \n",
    "        \n",
    "        # save loss for future reweighting # \n",
    "        score[order] = loss_train[client][-1] ** BETA_\n",
    "    ###################################\n",
    "    ####### dynamic weighting #########\n",
    "    ###################################\n",
    "    denominator = sum(score)\n",
    "    score = [s/denominator for s in score]\n",
    "    \n",
    "    denominator = sum(DATA_NUM)\n",
    "    WEIGHTS_CL = [s/denominator for s in DATA_NUM]\n",
    "    for order, _ in enumerate(WEIGHTS):\n",
    "        WEIGHTS[order] = WEIGHTS_CL[order] + LAMBDA_ * score[order]\n",
    "        \n",
    "    ### normalize #####################\n",
    "    denominator = sum(WEIGHTS)\n",
    "    WEIGHTS = [w/denominator for w in WEIGHTS]\n",
    "\n",
    "    ###################################\n",
    "    ####### aggregation ###############\n",
    "    ###################################\n",
    "    aggr_fed(CLIENTS, WEIGHTS, nets, fed_name='global')\n",
    "    aggr_fed(CLIENTS_2, WEIGHTS, nets, fed_name='global_2')\n",
    "    ################### test ##############################\n",
    "    avg_acc = 0.0\n",
    "    for client in CLIENTS:\n",
    "        test_multiclasses(epoch, testing_clients[client], nets[client], device, acc_test[client],\\\n",
    "             loss_test[client])\n",
    "        avg_acc += acc_test[client][-1]\n",
    "        \n",
    "    avg_acc = avg_acc / TOTAL_CLIENTS\n",
    "    ############################################################\n",
    "    ########################################################\n",
    "    if avg_acc > best_avg_acc:\n",
    "        best_avg_acc = avg_acc\n",
    "        best_epoch = epoch\n",
    "    \n",
    "    ################################\n",
    "    # plot #########################\n",
    "    ################################\n",
    "    clear_output(wait=True)\n",
    "    print(avg_acc, best_avg_acc)\n",
    "    print(WEIGHTS)\n",
    "    plot_graphs(0, CLIENTS, index, acc_train, 'acc_train')\n",
    "    plot_graphs(1, CLIENTS, index, loss_train, 'loss_train')\n",
    "    plot_graphs(2, CLIENTS, index, acc_test, ' acc_test')\n",
    "\n",
    "print(best_avg_acc, best_epoch)\n",
    "for client in CLIENTS:\n",
    "    print(client)\n",
    "    tmp = best_epoch\n",
    "    best_epoch = best_epoch \n",
    "    print(\"shared epoch specific\")\n",
    "    print(acc_test[client][best_epoch])\n",
    "    print(\"max client-specific\")\n",
    "    print(np.max(acc_test[client]))\n",
    "    best_epoch = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7967748a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DeepLearning)",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
