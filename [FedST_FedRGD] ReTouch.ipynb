{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f014caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from unet import UNet\n",
    "from dice_loss import dice_coeff\n",
    "####################################################\n",
    "# for data splitting\n",
    "####################################################\n",
    "import pandas as pd\n",
    "####################################################\n",
    "# for data preparation\n",
    "####################################################\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, balanced_accuracy_score\n",
    "####################################################\n",
    "# for plotting\n",
    "####################################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "############################\n",
    "# Helper func\n",
    "############################\n",
    "from helper import * \n",
    "\n",
    "########################################\n",
    "N_CHANNELS = 1 #greyscale\n",
    "N_CLASSES = 3 # classes, IRF, SRF, PED\n",
    "\n",
    "BATCH_SIZE, EPOCHS = 16, 150\n",
    "IMAGE_SIZE = (224, 224)\n",
    "CROP_SIZE = (200, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268f6163",
   "metadata": {},
   "outputs": [],
   "source": [
    "PTH = 'dataset/retouch/mask/'\n",
    "PTH_IM = 'dataset/retouch/slices/'\n",
    "\n",
    "Cirrus_ = np.arange(1,25)\n",
    "Spectralis_ = np.arange(25,49)\n",
    "Topcon_ = np.arange(49,71)\n",
    "\n",
    "cirrus_test = np.random.choice(Cirrus_, size=5, replace=False)\n",
    "cirrus_test = [str(a) for a in cirrus_test]\n",
    "cirrus_train = [str(a) for a in Cirrus_ if str(a) not in cirrus_test]\n",
    "print(len(cirrus_train)/len(Cirrus_))\n",
    "\n",
    "spectralis_test = np.random.choice(Spectralis_, size=5, replace=False)\n",
    "spectralis_test = [str(a) for a in spectralis_test]\n",
    "spectralis_train = [str(a) for a in Spectralis_ if str(a) not in spectralis_test]\n",
    "print(len(spectralis_train)/len(Spectralis_))\n",
    "\n",
    "topcon_test = np.random.choice(Topcon_, size=5, replace=False)\n",
    "topcon_test = [str(a) for a in topcon_test]\n",
    "topcon_train = [str(a) for a in Topcon_ if str(a) not in topcon_test]\n",
    "print(len(topcon_train)/len(Topcon_))\n",
    "\n",
    "whole_data =  os.listdir(PTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feefc4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SPLIT = dict()\n",
    "TEST_SPLIT['Cirrus'] = cirrus_test\n",
    "TEST_SPLIT['Topcon'] = topcon_test\n",
    "TEST_SPLIT['Spectralis'] = spectralis_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34d0dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "WHOLE_DATA_TRAIN = dict()\n",
    "WHOLE_DATA_TEST = dict()\n",
    "\n",
    "WHOLE_DATA_TRAIN['Cirrus'] = []\n",
    "WHOLE_DATA_TRAIN['Topcon'] = []\n",
    "WHOLE_DATA_TRAIN['Spectralis'] = []\n",
    "\n",
    "WHOLE_DATA_TEST['Cirrus'] = []\n",
    "WHOLE_DATA_TEST['Topcon'] = []\n",
    "WHOLE_DATA_TEST['Spectralis'] = []\n",
    "\n",
    "for item in whole_data:\n",
    "    separator = item.split('_') # identify the source\n",
    "    sample_number = separator[1].split('0')[-1]\n",
    "    \n",
    "    '''\n",
    "    Check the item label\n",
    "    '''\n",
    "    label_slice = np.unique(np.array(Image.open(PTH+item)))\n",
    "    if len(label_slice) <=1:\n",
    "        continue\n",
    "    \n",
    "    if sample_number in TEST_SPLIT[separator[0]]:\n",
    "        # testing set\n",
    "        data_path = (PTH_IM+item, PTH+item) #x,y source\n",
    "        WHOLE_DATA_TEST[separator[0]].append(data_path)\n",
    "    else:\n",
    "        data_path = (PTH_IM+item, PTH+item) #x,y source\n",
    "        WHOLE_DATA_TRAIN[separator[0]].append(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab83d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_SPECTRALIS = 0\n",
    "print('name\\t\\t train\\t test')\n",
    "for item in WHOLE_DATA_TRAIN:\n",
    "    if item == 'Spectralis':\n",
    "        print(item,'\\t', len(WHOLE_DATA_TRAIN[item]),'\\t', len(WHOLE_DATA_TEST[item]))\n",
    "        LEN_SPECTRALIS = WHOLE_DATA_TRAIN[item]\n",
    "    else:\n",
    "        print(item,'\\t\\t', len(WHOLE_DATA_TRAIN[item]),'\\t', len(WHOLE_DATA_TEST[item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67948c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "WEIGHTS_CL = [0.0,0.0,0.0]\n",
    "CLIENTS = ['Cirrus', 'Topcon', 'Spectralis']\n",
    "CLIENTS_2 = [cl +'_2' for cl in CLIENTS]\n",
    "TOTAL_CLIENTS = len(CLIENTS)\n",
    "\n",
    "LR = 1.5e-3\n",
    "WD = 1e-5\n",
    "TH = 0.9\n",
    "\n",
    "LAMBDA_ =2\n",
    "BETA_=3\n",
    "TH = 0.9\n",
    "\n",
    "for idx, client in enumerate(WHOLE_DATA_TRAIN):\n",
    "    WEIGHTS_CL[idx] = len(WHOLE_DATA_TRAIN[client])\n",
    "\n",
    "    \n",
    "total_weight = sum(WEIGHTS_CL)\n",
    "WEIGHTS_CL = [s/total_weight for s in WEIGHTS_CL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8144b2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENTS_SUPERVISION = ['labeled', 'labeled', 'labeled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4ad07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = dict()\n",
    "for cl in CLIENTS:\n",
    "    split_dataset[cl+'_train'] = retouch(WHOLE_DATA_TRAIN[cl], train=True)\n",
    "    split_dataset[cl+'_test'] = retouch(WHOLE_DATA_TEST[cl], train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777da8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_CLIENTS = 3\n",
    "GLOBAL_ACC = 0.0\n",
    "\n",
    "training_clients, testing_clients = dict(), dict()\n",
    "########## aditional #####################\n",
    "training_clients_pl = dict()\n",
    "acc_train, acc_test, loss_train, loss_test = dict(), dict(), \\\n",
    "                                            dict(), dict()\n",
    "\n",
    "nets, optimizers = dict(), dict()\n",
    "\n",
    "nets['global'] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, \\\n",
    "                      bilinear=True).to(device)\n",
    "\n",
    "nets['global_2'] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, \\\n",
    "                      bilinear=True).to(device)\n",
    "\n",
    "for client, c_sup in zip(CLIENTS, CLIENTS_SUPERVISION):\n",
    "    if c_sup == 'labeled':\n",
    "        training_clients[client] = DataLoader(split_dataset[client+'_train'], batch_size=16,\\\n",
    "                     shuffle=True, num_workers=8)\n",
    "        training_clients_pl[client] = DataLoader(split_dataset[client+'_train'], batch_size=1, \\\n",
    "                    shuffle=True, num_workers=8)\n",
    "    else:\n",
    "        training_clients[client] = DataLoader(split_dataset[client+'_train'], batch_size=16,\\\n",
    "                             shuffle=True, num_workers=8)\n",
    "        ################# additional dataloader ##########################################\n",
    "        training_clients_pl[client] = DataLoader(split_dataset[client+'_train'], batch_size=1,\\\n",
    "                             shuffle=True, num_workers=8)\n",
    "        training_clients_pl[client+'_2'] = DataLoader(split_dataset[client+'_train'], batch_size=1,\\\n",
    "                             shuffle=True, num_workers=8)\n",
    "    ###################################################################################\n",
    "    testing_clients[client] = DataLoader(split_dataset[client+'_test'], batch_size=16,\\\n",
    "                         shuffle=False, num_workers=1)\n",
    "    \n",
    "    acc_train[client], acc_test[client] = [], []\n",
    "    loss_train[client], loss_test[client] = [], []\n",
    "        \n",
    "    nets[client] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, \\\n",
    "                      bilinear=True).to(device)\n",
    "    nets[client+'_2'] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, \\\n",
    "                      bilinear=True).to(device)\n",
    "    optimizers[client]= optim.Adam(nets[client].parameters(), \\\n",
    "                                   lr=LR,weight_decay=WD)\n",
    "    optimizers[client+'_2']= optim.Adam(nets[client+'_2'].parameters(), \\\n",
    "                                   lr=LR,weight_decay=WD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7805103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0672eaaf",
   "metadata": {},
   "source": [
    "# FedST or FedRGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe7762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_POSTWARMUP = [0.1/2, 0.1/2, 0.9] #put more weight to client with strong supervision\n",
    "WARMUP_EPOCH = 100\n",
    "CLIENTS_SUPERVISION = ['unlabeled', 'unlabeled', 'labeled']\n",
    "\n",
    "\n",
    "MODE = 'FedST' \n",
    "# MODE = 'FedRGD'\n",
    "\n",
    "CE_ = nn.BCELoss()\n",
    "WEIGHTS = [0.0, 0.0, 1.0]\n",
    "\n",
    "if MODE == 'FedST':\n",
    "    WEIGHTS_POSTWARMUP = [0.1/2, 0.1/2, 0.9] #put more weight to client with strong supervision\n",
    "else:\n",
    "    WEIGHTS_POSTWARMUP = [0.5/2, 0.5/2, 0.5] #random client, but effectively, put 1/2 trust to strongly supervised client\n",
    "    \n",
    "CE_ = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e8fc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_avg_acc, best_epoch_avg = 0,0\n",
    "index = []\n",
    "\n",
    "for client in CLIENTS:\n",
    "    acc_train[client], acc_test[client] = [], []\n",
    "    loss_train[client], loss_test[client] = [], [] \n",
    "USE_UNLABELED_CLIENT = False    \n",
    "for epoch in range(EPOCHS):\n",
    "    index.append(epoch)\n",
    "    if epoch == WARMUP_EPOCH:\n",
    "        WEIGHTS = WEIGHTS_POSTWARMUP\n",
    "        USE_UNLABELED_CLIENT = True\n",
    "    ####### conduct training #####\n",
    "    #################### copy fed model ###################\n",
    "    copy_fed(CLIENTS, nets, fed_name='global')\n",
    "    \n",
    "    for client, supervision_t in zip(CLIENTS, CLIENTS_SUPERVISION):\n",
    "        if supervision_t == 'unlabeled':\n",
    "            if not USE_UNLABELED_CLIENT:\n",
    "                acc_train[client].append(0)\n",
    "                loss_train[client].append(0)\n",
    "                continue\n",
    "        print(client)\n",
    "        train_model_multiclasses(training_clients[client], nets[client], \\\n",
    "                                  optimizers[client], device, \\\n",
    "                                  acc = acc_train[client], \\\n",
    "                                  loss = loss_train[client], \\\n",
    "                                  supervision_type = supervision_t,\\\n",
    "                                warmup=True, CE_LOSS=CE_)\n",
    "        \n",
    "    aggr_fed(CLIENTS, WEIGHTS, nets, fed_name='global')\n",
    "    ################### test ##############################\n",
    "    avg_acc = 0.0\n",
    "    for client in CLIENTS:\n",
    "        test_multiclasses(epoch, testing_clients[client], nets[client], device, acc_test[client],\\\n",
    "             loss_test[client])\n",
    "        avg_acc += acc_test[client][-1]\n",
    "        \n",
    "    avg_acc = avg_acc / TOTAL_CLIENTS\n",
    "    ############################################################\n",
    "    ########################################################\n",
    "    if avg_acc > best_avg_acc:\n",
    "        best_avg_acc = avg_acc\n",
    "        best_epoch = epoch\n",
    "    \n",
    "    ################################\n",
    "    # plot #########################\n",
    "    ################################\n",
    "    clear_output(wait=True)\n",
    "    print(avg_acc, best_avg_acc)\n",
    "    plot_graphs(0, CLIENTS, index, acc_train, 'acc_train')\n",
    "    plot_graphs(1, CLIENTS, index, loss_train, 'loss_train')\n",
    "    plot_graphs(2, CLIENTS, index, acc_test, ' acc_test')\n",
    "\n",
    "print(best_avg_acc, best_epoch)\n",
    "for client in CLIENTS:\n",
    "    print(client)\n",
    "    tmp = best_epoch\n",
    "    best_epoch = best_epoch \n",
    "    print(\"shared epoch specific\")\n",
    "    print(acc_test[client][best_epoch])\n",
    "    print(\"max client-specific\")\n",
    "    print(np.max(acc_test[client]))\n",
    "    best_epoch = tmp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DeepLearning)",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
